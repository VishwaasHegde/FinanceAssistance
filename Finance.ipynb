{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450e9905-028a-4ba4-877a-6649c8fac7d8",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "1. Install [Ollama](https://ollama.com/download)\n",
    "2. In the command prompt: run `ollama pull llama3.2` and optionally `ollama serve`\n",
    "3. git clone the project\n",
    "4. Create a conda virtual environment and `pip install -r requirements.txt`\n",
    "5. Place [qdrant]() folder in the project folder\n",
    "7. Place [peft-sent-model]() folder in `src/backend` folder\n",
    "8. Place [peft-sent-model]() folder in the Project folder (This is for the jupyter notebook)\n",
    "\n",
    "### Install Qdrant\n",
    "1. Install [Docker](https://docs.docker.com/desktop/setup/install/windows-install/)\n",
    "2. In the project folder, run : `docker pull qdrant/qdrant`\n",
    "3. The run `docker run -p 6333:6333 -p 6334:6334 -v ./qdrant_storage:/qdrant/storage:z qdrant/qdrant`\n",
    "\n",
    "### For the UI / backend code (Not Jupyter)\n",
    "1. Do the steps above (from 1 to 6) and Qdrant installation\n",
    "2. Execute `uvicorn financeengine:app --host 0.0.0.0 --port 8025` in `src/backend` folder (in another terminal)\n",
    "3. Execute `streamlit run financechatclient.py` in `src/frontend` folder (in another terminal)\n",
    "4. Open `http://localhost:8501/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af3fc18-b545-4f3e-8f3b-8ee3c64b0359",
   "metadata": {},
   "source": [
    "### Imports\n",
    "These are mainly transformers, llama_index for vector store, pandas, qdrant etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16ed9320-b7ce-450e-8ec9-78a9b3562bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from llama_index.core import Settings\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, Document, StorageContext\n",
    "import torch\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display, HTML\n",
    "import abc\n",
    "from typing import List, Dict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import Trainer\n",
    "from torch.utils.data import Subset\n",
    "import requests\n",
    "from fastapi import FastAPI\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from llama_index.core.base.llms.types import ChatMessage, ChatResponse, MessageRole\n",
    "from transformers import AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cace38-9609-4bd1-ab3c-25c8c7f24bac",
   "metadata": {},
   "source": [
    "#### Run the below if you dont have nltk punkt (uncomment) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c9b9238-227d-4271-9650-daac2ae87baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e45435-11e8-46c6-8609-070f4ebbeb75",
   "metadata": {},
   "source": [
    "### The below code sets the environment and llm used for RAG\n",
    "\n",
    "1. We first set the base model. You can even try other ollama models like tinyllama, llama3.1 etc\n",
    "2. Then we set the [embedding model](https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings/)\n",
    "   1. This model is used for calculating similarity and top-k retrieval\n",
    "3. We also set the [chunk size](https://docs.llamaindex.ai/en/stable/optimizing/basic_strategies/basic_strategies/#chunk-sizes)\n",
    "4. Also see [this post](https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5) about setting the right chunk size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d799d9fc-cd9a-4ec7-b0f4-62647d99919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base_model = 'llama3.2'\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name='llmrails/ember-v1'\n",
    ") # Refer\n",
    "\n",
    "Settings.chunk_size = 2048\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4f61a-1b36-43fa-b982-39ab2d95c23a",
   "metadata": {},
   "source": [
    "We also do `PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0`. This makes pytorch use all the available data. Without this, you may get an error: `MPS backend out of memory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d84f80-949f-450e-a077-a213e7416f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mac and Linux\n",
    "!export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
    "# For Windows\n",
    "# !set PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61101bd0-bd6c-416b-bd50-51e4e2655f55",
   "metadata": {},
   "source": [
    "### The below code instantiates a class that handles Qdrant DB used for building RAG\n",
    "1. We first instantiate a QdrantClient that acts as a cliet to communicate with the Qdrant DB\n",
    "2. In the `index_data` method, we create a colletion using `QdrantVectorStore`, this is designed for storing and retrieving vector embeddings\n",
    "3. We then create a StorageContext that manages QdrantVectorStore\n",
    "4. We use a `try..except` block, because the some documents can be bigger than our chunk size, in which case, we simply drop the document\n",
    "\n",
    "### [QdrantVectorStore](https://docs.llamaindex.ai/en/stable/examples/vector_stores/QdrantIndexDemo/)\n",
    "Purpose: It is a direct interface to the Qdrant vector database, which is designed for storing and retrieving vector embeddings.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Stores vector embeddings and associated metadata.\n",
    "Manages interaction with the Qdrant database, including CRUD operations on collections, vectors, and payloads.\n",
    "Facilitates vector similarity searches to retrieve the most relevant vectors based on a query embedding.\n",
    "\n",
    "### [StorageContext](https://docs.llamaindex.ai/en/stable/api_reference/storage/storage_context/)\n",
    "Purpose: This is a higher-level abstraction used in frameworks like llama_index to manage various storage backends (including vector stores) seamlessly.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Acts as a bridge between data (e.g., documents, embeddings) and specific storage implementations (e.g., Qdrant, Weaviate, in-memory storage).\n",
    "Provides a unified interface to access and manipulate data without being tied to a specific backend.\n",
    "May combine data sources, such as documents in a database and vector embeddings, into a single context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99c26f35-557b-482b-bf49-99cfa1cfc016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QdrantHandler:\n",
    "    def __init__(self, collection_name='finance_collection', host='localhost', port=6333):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.qdrant_client = QdrantClient(host=self.host, port=self.port) #Instantiate Qdrant DB with given port and host\n",
    "        self.collection_name=collection_name #Name of the Qdrant collection\n",
    "        \n",
    "    def index_data(self, llama_documents):\n",
    "        vector_store = QdrantVectorStore(client=self.qdrant_client, collection_name=self.collection_name)\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        # print(llama_documents)\n",
    "        index = VectorStoreIndex([]) \n",
    "        for doc in tqdm(llama_documents[:]):\n",
    "            try:\n",
    "                index.insert(doc)\n",
    "            except (ValueError, RuntimeError) as v:\n",
    "                print(f'Skipped: {v}')\n",
    "\n",
    "    def get_qdrant_client(self):\n",
    "        return self.qdrant_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ee10c-896e-42ae-b2f1-d10a71a268bf",
   "metadata": {},
   "source": [
    "### The below code creates a class to extract information from news data\n",
    "1. We extract relavant information from [US_Financial_News_Articles](https://drive.google.com/file/d/1m2FAtyA_NvWsZ_V3OhHl0JgTGkDGp6kp/view?usp=sharing)\n",
    "2. This data contains several fields out of which we only fetch information such as location, title of the article, published date, news content\n",
    "3. Other details such as author name, website name, article url etc are skipped, since they are not important for our task\n",
    "4. With the extracted information we create a text embedding, and append the text and its embedding in the form of `Document`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b7fb1da-013a-4d2e-be10-2db4ab21ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that handles pre-processing of US News data\n",
    "\n",
    "class DataExtractionBaseClass(abc.ABC):\n",
    "    def __init__(self, qdrant_client):\n",
    "        self.qdrant_client = qdrant_client\n",
    "\n",
    "    abc.abstractmethod\n",
    "    def extract_data(self, *vargs) -> List[Document]:\n",
    "        return \n",
    "\n",
    "class USNewsDataExtraction(DataExtractionBaseClass):\n",
    "    def __init__(self, qdrant_client):\n",
    "        super().__init__(qdrant_client)\n",
    "\n",
    "    def get_locations(self, json_data):\n",
    "        locations_list = json_data['entities']['locations']\n",
    "        locations = []\n",
    "        for ld in locations_list:\n",
    "            if ('name' in ld) and ld['name'].strip()!='':\n",
    "                locations.append(ld['name'])    \n",
    "    \n",
    "        if locations:\n",
    "            return ', '.join(locations)\n",
    "    \n",
    "        return ''\n",
    "\n",
    "    def json_2_text(self, json_data):\n",
    "        title = json_data['title']\n",
    "        location = self.get_locations(json_data)\n",
    "        \n",
    "        title_template = f'The title of the news article is {title}.'\n",
    "        if location:\n",
    "            location_template = f'The locations relavant to the article are: {location}.'\n",
    "        else:\n",
    "            location_template = ''\n",
    "        published = json_data['published'][:10]\n",
    "        date_template = f'This article was published on {published}'\n",
    "        article = json_data['text']\n",
    "        article_template = f'Article: {article}'\n",
    "    \n",
    "        item = {'title': title, 'location': location, 'published':  published, 'article': article}\n",
    "        text = ' '.join([title_template, location_template, date_template, article_template])\n",
    "        return text, item\n",
    "\n",
    "    def extract_data(self, qdrant_client, number_of_folders=4, number_of_files_per_folder=10000, collection_name='finance_collection'):\n",
    "        base_folder = 'data/US_Financial_News_Articles'\n",
    "        c=0\n",
    "        llama_documents = []\n",
    "        for folder_name in os.listdir(base_folder)[:number_of_folders]:\n",
    "            folder_path = os.path.join(base_folder, folder_name)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "            for file_name in tqdm(os.listdir(folder_path)[:number_of_files_per_folder]):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    text, item = json_2_text(json_data)\n",
    "                    point_id = str(uuid.uuid4())\n",
    "                    vector = Settings.embed_model.get_text_embedding(text)\n",
    "                    point = {\n",
    "                    \"id\": point_id,\n",
    "                    \"vector\": vector,\n",
    "                    \"payload\": {\n",
    "                        \"title\": item[\"title\"],\n",
    "                        \"location\": item[\"location\"],\n",
    "                        \"published\": item[\"published\"],\n",
    "                        \"article\": item[\"article\"],\n",
    "                    }\n",
    "                    }\n",
    "                c+=1\n",
    "    \n",
    "                document = Document(metadata=item, text=text)\n",
    "                llama_documents.append(document)\n",
    "                # qdrant_client.upsert(collection_name=collection_name, points=[point])\n",
    "        return llama_documents\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1840e40-6f37-4701-9137-aa771d64e554",
   "metadata": {},
   "source": [
    "Sometimes creating a collection when that collection name already exists might lead to unexpected results, its a good idea to delete collection if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c10c4476-c6e6-4ae4-a048-da6d094c710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete old collection if necessary\n",
    "# client.delete_collection('finance_collection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f5696-34a8-4208-b03c-9a412348147d",
   "metadata": {},
   "source": [
    "### Create an instance of Qdranthandler and get qdrant client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd58055c-13f1-476f-81b6-a538b9ae09f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "qdrant_handler = QdrantHandler()\n",
    "qdrant_client = qdrant_handler.get_qdrant_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d1286-962f-46df-88dc-0fab845fe5c2",
   "metadata": {},
   "source": [
    "### Instantiate US News data extractor and create news - embedding pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8a89d6f8-898c-41cf-874a-259eb44984d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [3:43:21<00:00,  3.73it/s]\n",
      "100%|███████████████████████████████████| 50000/50000 [3:12:35<00:00,  4.33it/s]\n",
      "100%|███████████████████████████████████| 50000/50000 [2:43:11<00:00,  5.11it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_extractor_us = USNewsDataExtraction(qdrant_client)\n",
    "llama_documents = data_extractor_us.extract_data(qdrant_client, 4, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815e82e-ba8a-4430-be55-8421ca124727",
   "metadata": {},
   "source": [
    "### Index the data; this will create a qdrant vector store later used for retrieval, as mentioned before some files that are too big are skipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "60f1e8bb-4f59-4e90-8613-2f67e5ecda7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 10/150000 [00:06<29:15:51,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: Metadata length (4630) is longer than chunk size (2048). Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 102/150000 [00:56<22:37:04,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: Metadata length (7425) is longer than chunk size (2048). Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 164/150000 [02:01<20:48:50,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: Metadata length (4348) is longer than chunk size (2048). Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
      "Metadata length (2041) is close to chunk size (2048). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                  | 166/150000 [02:39<372:00:13,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: MPS backend out of memory (MPS allocated: 3.18 GB, other allocations: 3.44 GB, max allowed: 6.77 GB). Tried to allocate 160.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 171/150000 [02:41<89:15:23,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: MPS backend out of memory (MPS allocated: 3.03 GB, other allocations: 3.72 GB, max allowed: 6.77 GB). Tried to allocate 32.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 192/150000 [02:49<12:00:16,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: Metadata length (3129) is longer than chunk size (2048). Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 193/150000 [02:49<11:57:08,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: Metadata length (2438) is longer than chunk size (2048). Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 211/150000 [02:58<12:52:04,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: Metadata length (11115) is longer than chunk size (2048). Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                  | 283/150000 [03:49<348:47:47,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: MPS backend out of memory (MPS allocated: 3.05 GB, other allocations: 3.62 GB, max allowed: 6.77 GB). Tried to allocate 160.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 333/150000 [04:03<7:33:58,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: Metadata length (2404) is longer than chunk size (2048). Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 355/150000 [04:11<10:19:02,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: Metadata length (2548) is longer than chunk size (2048). Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 366/150000 [04:16<16:19:10,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: Metadata length (2190) is longer than chunk size (2048). Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 381/150000 [04:26<29:05:38,  1.43it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[222], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mqdrant_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama_documents\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[220], line 15\u001b[0m, in \u001b[0;36mQdrantHandler.index_data\u001b[0;34m(self, llama_documents)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(llama_documents[:]):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m         \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m v:\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSkipped: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/llama_index/core/indices/base.py:246\u001b[0m, in \u001b[0;36mBaseIndex.insert\u001b[0;34m(self, document, **insert_kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsert\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    241\u001b[0m         [document],\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformations,\n\u001b[1;32m    243\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_progress,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocstore\u001b[38;5;241m.\u001b[39mset_document_hash(document\u001b[38;5;241m.\u001b[39mget_doc_id(), document\u001b[38;5;241m.\u001b[39mhash)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:337\u001b[0m, in \u001b[0;36mVectorStoreIndex.insert_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m             node\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsert_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:318\u001b[0m, in \u001b[0;36mVectorStoreIndex._insert\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_insert\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes: Sequence[BaseNode], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Insert a document.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_nodes_to_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:238\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[0;32m--> 238\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_with_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39madd(nodes_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:145\u001b[0m, in \u001b[0;36mVectorStoreIndex._get_node_with_embedding\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_node_with_embedding\u001b[39m(\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    135\u001b[0m     nodes: Sequence[BaseNode],\n\u001b[1;32m    136\u001b[0m     show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    137\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    Get tuples of id, node, and embedding.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     id_to_embed_map \u001b[38;5;241m=\u001b[39m \u001b[43membed_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/llama_index/core/indices/utils.py:138\u001b[0m, in \u001b[0;36membed_nodes\u001b[0;34m(nodes, embed_model, show_progress)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         id_to_embed_map[node\u001b[38;5;241m.\u001b[39mnode_id] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39membedding\n\u001b[0;32m--> 138\u001b[0m new_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_embedding_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts_to_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[1;32m    143\u001b[0m     id_to_embed_map[new_id] \u001b[38;5;241m=\u001b[39m text_embedding\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:260\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    253\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[1;32m    254\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/llama_index/core/base/embeddings/base.py:332\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding_batch\u001b[0;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    324\u001b[0m     EmbeddingStartEvent(\n\u001b[1;32m    325\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39mmodel_dict,\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    329\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING,\n\u001b[1;32m    330\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()},\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 332\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     result_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    334\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    335\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    336\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: cur_batch,\n\u001b[1;32m    337\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: embeddings,\n\u001b[1;32m    338\u001b[0m         },\n\u001b[1;32m    339\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/llama_index/embeddings/huggingface/base.py:150\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding._get_text_embeddings\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_text_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    149\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get text embeddings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/llama_index/embeddings/huggingface/base.py:125\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding._embed\u001b[0;34m(self, sentences, prompt_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed\u001b[39m(\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    121\u001b[0m     sentences: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    122\u001b[0m     prompt_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    123\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed sentences.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:621\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 621\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    623\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:688\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    687\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:350\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    348\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    353\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:554\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    552\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    553\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> 554\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/modules/normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_1/lib/python3.10/site-packages/torch/nn/functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2545\u001b[0m     )\n\u001b[0;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "qdrant_handler.index_data(llama_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b9bb9-f5e7-4f3a-8d0e-6ea0de757333",
   "metadata": {},
   "source": [
    "### The below creates a class to help in retrieval\n",
    "1. We create a custom class to set a score_threhold, this decides the threshold / cosine similarity that we would like to set while retrieving the documents\n",
    "2. Higher the threshold more the filtering\n",
    "3. We also set how many matches to fetch using `similarity_top_k`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15715a93-bf0b-4d2d-a35d-0ba063401c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a Qdrant service - used by the model\n",
    "\n",
    "class CustomQdrantClient:\n",
    "    def __init__(self, client) -> None:\n",
    "        self._client: QdrantClient = client\n",
    "\n",
    "    def collection_exists(self, c):\n",
    "        return self._client.collection_exists(c)\n",
    "\n",
    "    def search(self, collection_name, query_vector, limit, query_filter):\n",
    "        return self._client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=limit,\n",
    "            query_filter=query_filter,\n",
    "            score_threshold = 0.3\n",
    "        )\n",
    "\n",
    "class QdrantService:\n",
    "    def __init__(self, client, collection_name='finance_collection'):\n",
    "        self.collection_name = collection_name\n",
    "        self.custom_qdrant_client = CustomQdrantClient(client)\n",
    "\n",
    "    def get_vector_store_index(self):\n",
    "\n",
    "        if not self.custom_qdrant_client.collection_exists(c=self.collection_name):\n",
    "            self.logger.warning(f\"Collection {self.collection_name} does not exist !\")\n",
    "            return None\n",
    "\n",
    "        vector_store = QdrantVectorStore(\n",
    "            client=self.custom_qdrant_client,\n",
    "            collection_name=self.collection_name,\n",
    "            parallel=1,\n",
    "        )\n",
    "        index: VectorStoreIndex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "        query_engine: BaseQueryEngine = index.as_query_engine(\n",
    "            similarity_top_k=5,\n",
    "            verbose=False, streaming=False\n",
    "        )\n",
    "        return query_engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766474ff-e376-4533-8133-e52f5b13df78",
   "metadata": {},
   "source": [
    "#### Pydantic class - for data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80149249-bd2a-45a8-a3ee-143bfa37e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e9d2ab-3936-4fc8-94ca-d29fc81b4980",
   "metadata": {},
   "source": [
    "#### Main class for QA\n",
    "1. We create instances for Qdrant service (DB), Vector store and `historical_messages` which helps keep track of the historical chat up to 3 conversations\n",
    "2. `get_model_instance` method fetches the instance of Ollama, `base_model_name` can only be a Ollama model like llama3, tinyllama etc\n",
    "3.  For finetuned model, we create `peft_base_model` and its tokenizer `peft_tokenizer`\n",
    "4.  We create a `prompt`, this is used for all messages to the model\n",
    "5.  We then create the instance of Qdrant Service DB, and vector index\n",
    "6.  Then using `peft_base_model` we create the Peft model\n",
    "7.  The `llm_request` method takes the input query and model_type\n",
    "8.  The model_type can ['Pure LLM', 'LLM + RAG', 'Finetuned LLM']\n",
    "9.  The `llm_request` method when run on 'LLM + RAG', tries to use the RAG, if no suitable text in the documents is found for the given query, it falls back to running simple LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a08e02e8-afd7-4b82-accf-fe6b57414a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FinanceQA:\n",
    "\n",
    "    def __init__(self, qdrant_client, base_model_name='llama3.2', peft_base_model_name='google/flan-t5-base'):\n",
    "        self.base_model_name = base_model_name\n",
    "        self.base_model: CustomLLM = self.get_model_instance()\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.peft_base_model_name = peft_base_model_name\n",
    "        self.historical_messages: List[Message] = []\n",
    "\n",
    "        self.peft_base_model = AutoModelForSeq2SeqLM.from_pretrained(peft_base_model_name, torch_dtype=torch.bfloat16)\n",
    "        self.peft_tokenizer = AutoTokenizer.from_pretrained(peft_base_model_name)\n",
    "        # self.llm_model: CustomLLM = self.get_model_instance()\n",
    "        self.prompt = 'You are Q&A assistant in Finance. Please provide a concise answer, up to 400 words. Focus on key points, avoid unnecessary details, and ensure important context is preserved.'\n",
    "        self.set_defaults()\n",
    "\n",
    "\n",
    "\n",
    "        self.qdrant_ser: QdrantService = QdrantService(qdrant_client)\n",
    "        self.vector_store_index: BaseQueryEngine = self.qdrant_ser.get_vector_store_index()\n",
    "\n",
    "\n",
    "        # PEFT\n",
    "        self.peft_model = PeftModel.from_pretrained(self.peft_base_model,\n",
    "                                               # './peft-dialogue-summary-checkpoint-local',\n",
    "                                                    './peft-sent-model',\n",
    "                                               torch_dtype=torch.bfloat16,\n",
    "                                               is_trainable=False)  ## is_trainable mean just a forward pass jsut to get a sumamry\n",
    "\n",
    "\n",
    "    def get_model_instance(self):\n",
    "        model_instance = Ollama(\n",
    "            # model = 'phi3:3.8b-mini-128k-instruct-q8_0',\n",
    "            model=self.base_model_name,\n",
    "            request_timeout=480,\n",
    "            temperature=0.3,\n",
    "            tokenizer_mode=\"slow\",\n",
    "            context_window=3000,\n",
    "            additional_kwargs={\n",
    "                'num_thread': 8,\n",
    "                'num_ctx': 2500,\n",
    "                'num_predict': 650,\n",
    "\n",
    "            },\n",
    "            base_url='http://localhost:11434')\n",
    "        return model_instance\n",
    "\n",
    "\n",
    "    def update_historical_context(self, message: Message) -> None:\n",
    "        if len(self.historical_messages) == 8:\n",
    "            # Remove 3rd and 4th element. that is, the 2nd question and answer pair\n",
    "            del self.historical_messages[3]\n",
    "            del self.historical_messages[3]\n",
    "\n",
    "        self.historical_messages.append(message)\n",
    "\n",
    "    def convert_to_format(self, messages):\n",
    "        chat_messages = []\n",
    "        for message in messages:\n",
    "            chat_message = ChatMessage(\n",
    "                role=message.role,  # Convert string to MessageRole enum\n",
    "                content=message.content\n",
    "            )\n",
    "            chat_messages.append(chat_message)\n",
    "        return chat_messages\n",
    "\n",
    "    def clear(self):\n",
    "        self.historical_messages.clear()\n",
    "\n",
    "    def set_defaults(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        Settings.llm = self.base_model\n",
    "        Settings.embed_model = HuggingFaceEmbedding(\n",
    "            model_name='llmrails/ember-v1', device=device\n",
    "        )\n",
    "        system_prompt_message = Message(content=self.prompt, role='system')\n",
    "        self.historical_messages.append(system_prompt_message)\n",
    "\n",
    "    def handle_finetuned(self, message: Message):\n",
    "        prompt = self.prompt + '\\n' + message.content\n",
    "        input_ids = self.peft_tokenizer(prompt, return_tensors='pt').input_ids\n",
    "        peft_model_outputs = self.peft_model.generate(input_ids=input_ids,\n",
    "                                                 generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "        peft_model_text_output = self.peft_tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "        response = ChatResponse(\n",
    "            message=ChatMessage(role=MessageRole.ASSISTANT, content=peft_model_text_output))\n",
    "        return peft_model_text_output\n",
    "\n",
    "\n",
    "\n",
    "    def llm_request(self, query: str, model_type):\n",
    "        message = Message(role='user', content=query)\n",
    "\n",
    "        if model_type == 'Finetuned LLM':\n",
    "            return self.handle_finetuned(message)\n",
    "        if model_type == 'LLM + RAG':\n",
    "            use_qdrant = True\n",
    "        else:\n",
    "            use_qdrant = False\n",
    "        self.update_historical_context(message)\n",
    "        vector_response: RESPONSE_TYPE | None = None\n",
    "        if use_qdrant and self.vector_store_index:\n",
    "            vector_response = self.vector_store_index.query(self.historical_messages[-1].content)\n",
    "            # self.logger.debug(\"vector index store response: {0} \\n\".format(vector_response))\n",
    "            print('vector_response', vector_response)\n",
    "        if (vector_response is None) or len(vector_response.source_nodes) <= 0:\n",
    "            messages: List[ChatMessage] = self.convert_to_format(self.historical_messages)\n",
    "            response_message: ChatResponse = self.base_model.chat(messages)\n",
    "        else:\n",
    "            response_message = ChatResponse(\n",
    "                message=ChatMessage(role=MessageRole.ASSISTANT, content=vector_response.response))\n",
    "        self.update_historical_context(Message(role=MessageRole.ASSISTANT, content=response_message.message.content))\n",
    "        return response_message.message.content\n",
    "\n",
    "    def get_all_outputs(self, query):\n",
    "        model_types = ['Pure LLM', 'LLM + RAG', 'Finetuned LLM']\n",
    "        for mt in model_types:\n",
    "            output = self.llm_request(query, mt)\n",
    "            self.clear()\n",
    "            print(f'{mt}: {output}')\n",
    "            print('###################################################################################')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e89bd834-c5c1-4939-ace9-a7089607ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_qa = FinanceQA(qdrant_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b795b61a-e5d1-435e-ba8f-b0eb0e895980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure LLM: The sentiment of the statement is neutral/informative. The language used is objective and descriptive, providing a factual connection between the transaction and the company's pulp and paper industry-related solutions without expressing any emotion or opinion.\n",
      "vector_response Neutral.\n",
      "LLM + RAG: Neutral.\n",
      "Finetuned LLM: positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "query = '''Give the sentiment of the following statement:\n",
    "\n",
    "This transaction will also rationalize our pulp and paper industry related solutions .\n",
    "\n",
    "Sentiment:'''\n",
    "\n",
    "finance_qa.get_all_outputs(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6427f9bf-9505-47cb-9567-096cea851fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure LLM: Donald Trump, the 45th President of the United States, had a significant role in US finance before his presidency. Here are some key aspects of his involvement:\n",
      "\n",
      "1. Real Estate Development: Trump built his business empire on real estate development, particularly in New York City. He developed and managed numerous high-profile properties, including the Trump Tower, Trump Plaza Hotel and Casino, and the Mar-a-Lago resort.\n",
      "2. Atlantic City Casinos: In the 1990s, Trump expanded his business interests to Atlantic City, where he built several casinos, including Trump Taj Mahal, Trump Marina, and the Trump Plaza Hotel and Casino. The casinos were highly successful, but ultimately led to financial difficulties for Trump.\n",
      "3. Trump Organization: As the chairman of the Trump Organization, a private company that manages his business interests, Trump was responsible for overseeing various aspects of his business empire, including real estate development, hospitality, and entertainment.\n",
      "4. Debt and Bankruptcy: In 1990, Trump's casino business filed for bankruptcy, which led to significant debt restructuring. He also faced financial difficulties with his Atlantic City casinos, which ultimately led to the sale of some properties.\n",
      "5. Financial Support from Russian Oligarchs: There have been allegations that Trump received significant financial support from Russian oligarchs, including Vladimir Putin's close associates, during the 2016 presidential campaign. These claims are disputed, but they highlight the complex web of international business relationships that Trump had before becoming president.\n",
      "\n",
      "As President, Trump's role in US finance was more focused on policy and regulatory issues, such as:\n",
      "\n",
      "1. Tax Reform: Trump signed the Tax Cuts and Jobs Act (TCJA) in 2017, which significantly reduced corporate tax rates and introduced new deductions for pass-through entities.\n",
      "2. Deregulation: The Trump administration implemented various deregulatory efforts, including the repeal of the Dodd-Frank Wall Street Reform and Consumer Protection Act, which aimed to reduce regulatory burdens on banks and other financial institutions.\n",
      "3. Monetary Policy: The Federal Reserve, led by Chairman Jerome Powell, played a key role in shaping monetary policy during Trump's presidency. The Fed implemented several interest rate cuts and quantitative easing measures to support the economy.\n",
      "\n",
      "Overall, Trump's involvement in US finance was shaped by his business experiences, including real estate development, casino operations, and international partnerships. As president, he focused on policy initiatives that aimed to promote economic growth, reduce regulatory burdens, and support financial institutions.\n",
      "vector_response The article does not explicitly state Trump's role in US finance. However, it mentions that he has repeatedly called for reform to shrink the \"huge\" annual trade deficit, which suggests that trade policy is an area where he has been involved. It also notes that he characterized America's relationship with other leading economies as a \" very one-sided and unfair one \" and \"just not sustainable\", implying that he has been vocal about US economic issues.\n",
      "LLM + RAG: The article does not explicitly state Trump's role in US finance. However, it mentions that he has repeatedly called for reform to shrink the \"huge\" annual trade deficit, which suggests that trade policy is an area where he has been involved. It also notes that he characterized America's relationship with other leading economies as a \" very one-sided and unfair one \" and \"just not sustainable\", implying that he has been vocal about US economic issues.\n",
      "Finetuned LLM: President of the United States\n"
     ]
    }
   ],
   "source": [
    "query = '''What was Trump's role in US finance?'''\n",
    "\n",
    "finance_qa.get_all_outputs(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8fea0-4f0c-4801-ab30-dac9942bca90",
   "metadata": {},
   "source": [
    "####################################################################################################\n",
    "## Finetuning LoRA\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436aa668-3619-4b73-b684-76b1d8356380",
   "metadata": {},
   "source": [
    "#### The below install maybe required if you are going to finetune it, if it works with the current environment its well and good. If it doesn't consider creating a new environment with the below requirements for finetuning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d810207-c8ba-42ca-8d70-45c836c742ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.27.2 \\\n",
    "\n",
    "!pip install torch==1.13.1 \\\n",
    "\n",
    "!pip install torchdata==0.5.1 \\\n",
    "\n",
    "!pip install loralib==0.1.1 \\\n",
    "\n",
    "!pip install peft==0.3.0 \n",
    "!pip install evaluate\n",
    "\n",
    "\n",
    "!pip install datasets==2.11.0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "ad744f7b-0a6e-48fc-910b-b1bf30dea789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch \n",
    "import time \n",
    "import evaluate  ## for calculating rouge score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import abc\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd1e4c-67c5-47a5-a81e-ed2445dab60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586e5af-2c05-49eb-9e33-a279cc227a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FinetuneDataHandler(abc.ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    abc.abstractmethod\n",
    "    def extract_data():\n",
    "        pass\n",
    "\n",
    "class FinQA:\n",
    "    def __init__(self):\n",
    "        self.dataset = load_dataset(\"ibm/finqa\")\n",
    "\n",
    "\n",
    "    def create_context(self, example):\n",
    "        # Combine pre_text, table, and post_text into a single string\n",
    "        pre_text = example.get(\"pre_text\", \"\")\n",
    "        table = example.get(\"table\", [])\n",
    "        post_text = example.get(\"post_text\", \"\")\n",
    "        # print(table)\n",
    "    \n",
    "        # print(table)\n",
    "        df = pd.DataFrame(table)\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.iloc[1:]\n",
    "        df = df.set_index(df.columns[0])\n",
    "        # display(df)\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "        table_str = df.to_dict('index')\n",
    "        # \n",
    "        # table1 = df.iloc[1:].to_dict('records')\n",
    "    \n",
    "        # print(table1)\n",
    "        # table_str = \"\\n\".join(\n",
    "        #     [\", \".join([f\"{key}: {value}\" for key, value in row.items()]) for row in table1]\n",
    "        # )\n",
    "        # print(table_str)\n",
    "    \n",
    "    \n",
    "        # Convert table into a readable format (e.g., rows of data)\n",
    "        # table_str = \"\\n\".join(\n",
    "        #     [\", \".join([f\"{key}: {value}\" for key, value in row.items()]) for row in table]\n",
    "        # )\n",
    "        \n",
    "        # Combine everything into a readable format\n",
    "        # print(table_str)\n",
    "        context = f\"Pre-text: {pre_text}\\nTable:\\n{table_str}\\nPost-text: {post_text}\"\n",
    "        return context\n",
    "    \n",
    "    def preprocess_function(self, example):\n",
    "        # Create context from the available fields\n",
    "        context = self.create_context(example)\n",
    "        \n",
    "        # Combine the context and the question\n",
    "        input_text = f\"{context}\\nQuestion: {example['question']}\\nAnswer:\"\n",
    "        \n",
    "        # The answer is the target output\n",
    "        output_text = example[\"answer\"]\n",
    "        \n",
    "        return {\"input\": input_text, \"output\": output_text}\n",
    "\n",
    "    def extract_dataset(self):\n",
    "        processed_dataset = self.dataset.map(self.preprocess_function, remove_columns=self.dataset[\"train\"].column_names)\n",
    "        return processed_dataset\n",
    "\n",
    "\n",
    "class FinSum:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data_path_1 = '/kaggle/input/finsum/temp'\n",
    "        self.data_path_2 = '/kaggle/input/finsum'\n",
    "\n",
    "    def extract_data(self):\n",
    "        input_column = ['document']\n",
    "        output_column = ['summary']\n",
    "        all_datasets = []\n",
    "        for fol in [self.data_path_1]:\n",
    "            for f in tqdm(os.listdir(fol)):\n",
    "                fp = os.path.join(fol, f)\n",
    "                df = pd.read_csv(fp)\n",
    "                all_datasets.append(df)\n",
    "        \n",
    "        all_datasets = pd.concat(all_datasets, axis=0).iloc[:20000]\n",
    "        all_datasets['input'] = all_datasets['document']\n",
    "        # all_datasets['input'] = all_datasets['input'].apply(lambda x: \"Summarize the following text: \" + x)\n",
    "        all_datasets['output'] = all_datasets['summary']\n",
    "        # all_datasets['output'] = all_datasets['output'].apply(lambda x: \"Summary: \" + x)\n",
    "        del all_datasets['document']\n",
    "        del all_datasets['summary']\n",
    "        dataset = Dataset.from_pandas(all_datasets)\n",
    "\n",
    "        # Split into train, validation, and test sets (optional)\n",
    "        train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "        train_valid_split = train_test_split['train'].train_test_split(test_size=0.1, seed=42)\n",
    "        \n",
    "        # Create a DatasetDict\n",
    "        dataset_dict = DatasetDict({\n",
    "            \"train\": train_valid_split['train'],\n",
    "            \"validation\": train_valid_split['test'],\n",
    "            \"test\": train_test_split['test'],\n",
    "        })\n",
    "        return dataset_dict\n",
    "\n",
    "\n",
    "class FinSent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data_path_1 = '/kaggle/input/finance-sentiment/Sentences_50Agree.txt'\n",
    "\n",
    "    def extract_data(self):\n",
    "        all_datasets = pd.read_csv('/kaggle/input/finance-sentiment/Sentences_50Agree.txt', encoding= \"ISO-8859-1\", sep='.@', header=None)\n",
    "        all_datasets.columns = ['input', 'output']\n",
    "        # all_datasets = pd.concat(all_datasets, axis=0).iloc[:20000]\n",
    "        # all_datasets['input'] = all_datasets['document']\n",
    "        # # all_datasets['input'] = all_datasets['input'].apply(lambda x: \"Summarize the following text: \" + x)\n",
    "        # all_datasets['output'] = all_datasets['summary']\n",
    "        # # all_datasets['output'] = all_datasets['output'].apply(lambda x: \"Summary: \" + x)\n",
    "        # del all_datasets['document']\n",
    "        # del all_datasets['summary']\n",
    "        dataset = Dataset.from_pandas(all_datasets)\n",
    "\n",
    "        # Split into train, validation, and test sets (optional)\n",
    "        train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "        train_valid_split = train_test_split['train'].train_test_split(test_size=0.1, seed=42)\n",
    "        \n",
    "        # Create a DatasetDict\n",
    "        dataset_dict = DatasetDict({\n",
    "            \"train\": train_valid_split['train'],\n",
    "            \"validation\": train_valid_split['test'],\n",
    "            \"test\": train_test_split['test'],\n",
    "        })\n",
    "        return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a2272-1f37-4652-97d2-3133617930d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_sent = FinSent()\n",
    "finsent_dataset = fin_sent.extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2b044-5d3b-4f45-8035-eb5e5035a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for more information of model https://huggingface.co/google/flan-t5-base\n",
    "model_name = 'google/flan-t5-base'\n",
    "\n",
    "# bfloat16 mean we are using the small version of flan-t5\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef6bae-3523-4379-9d93-701f18455044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f'trainable model parameters: {trainable_model_params}\\n \\\n",
    "            all model parameters: {all_model_params} \\n \\\n",
    "            percentage of trainable model parameters: {(trainable_model_params / all_model_params) * 100} %'\n",
    "\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757f18e-0fee-44b2-b30b-54a3ed6dae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeninze_function(example):\n",
    "    start_prompt = 'Give the sentiment of the following statement: \\n\\n'\n",
    "    end_prompt = '\\n\\nSentiment: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"input\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding='max_length', truncation=True, \n",
    "                                     return_tensors='pt').input_ids\n",
    "    example['labels'] = tokenizer(example['output'], padding='max_length', truncation=True, \n",
    "                                 return_tensors='pt').input_ids\n",
    "    \n",
    "    return example\n",
    "    \n",
    "# The Dataseta ctually contains 3 diff splits: train, validation, and test.\n",
    "# The tokenize_function code is handling all data across all splits in batches\n",
    "# tokenize_datasets = finsum_dataset.map(tokeninze_function, batched=True)\n",
    "tokenize_datasets = finsent_dataset.map(tokeninze_function, batched=True)\n",
    "# tokenize_datasets = tokenize_datasets.remove_columns(['id', 'topic', 'input',\n",
    "#                                                      'output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e16b51-51b1-43f1-9b41-428bd19e83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85866685-8ebf-409e-8f20-e136bd6f2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(r=32, #rank 32,\n",
    "                         lora_alpha=32, ## LoRA Scaling factor \n",
    "                         target_modules=['q', 'v'], ## The modules(for example, attention blocks) to apply the LoRA update matrices.\n",
    "                         lora_dropout = 0.3,\n",
    "                         bias='none',\n",
    "                         task_type=TaskType.SEQ_2_SEQ_LM ## flan-t5\n",
    ")\n",
    "\n",
    "## target_modules='q', This represents the value projection layer in the transformer model. The value projection layer transforms input tokens into value vectors,\n",
    "# which are the actual values that are attended to based on the attention scores computed from query and key vectors.\n",
    "\n",
    "## target_modules='v',This typically refers to the query projection layer in a transformer-based model. The query projection layer is responsible for transforming \n",
    "# input tokens into query vectors, which are used to attend to other tokens in the sequence during self-attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae8ccd-f75d-434a-a408-fc660eeee10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = original_model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acffe8e-0903-4d57-9291-ebbd4b4f5d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a2cb5-11cc-4786-8327-948b18bdfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = peft_model.to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556ec43-a011-4a22-a039-8943d7b087ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "## this is we are again back to the hugging face trainer module\n",
    "# training_args = TrainingArguments(output_dir=output_dir,\n",
    "#                                        evaluation_strategy=\"steps\",\n",
    "#                                        auto_find_batch_size=True,\n",
    "#                                        learning_rate=1e-3,\n",
    "#                                        num_train_epochs=1,\n",
    "#                                        # logging_steps=1,\n",
    "#                                        max_steps=5000,\n",
    "#                                        weight_decay=0.01,\n",
    "#                                        save_total_limit=0,\n",
    "#                                        # per_device_train_batch_size=1,\n",
    "#                                        label_names = ['labels'],\n",
    "#                                        save_steps=500,\n",
    "#                                         report_to='none', ## can be wandb, but we are reporint to noe\n",
    "                                       \n",
    "#                 )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=1,\n",
    "            num_train_epochs=1,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=\"./logs\",\n",
    "            save_total_limit=0,\n",
    "            save_steps=500,\n",
    "            # fp16=True,  # Use mixed precision for faster training\n",
    "            # use_cpu=False,\n",
    "            label_names = ['labels']\n",
    "        )\n",
    "\n",
    "## this is same except we are using PEFT model instead of regular\n",
    "\n",
    "peft_trainer = Trainer(model=peft_model, \n",
    "                      args=training_args,\n",
    "                      train_dataset=tokenize_datasets['train'],\n",
    "                      eval_dataset=tokenize_datasets['validation']\n",
    "                 )\n",
    "peft_trainer.args._n_gpu = 1\n",
    "\n",
    "\n",
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path = './peft-dialogue-summary-checkpoint-local'\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1bc007-f2b7-43ef-b75e-bd7446248572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                      './peft-sent-model',\n",
    "                                      torch_dtype=torch.bfloat16,\n",
    "                                      is_trainable=False) ## is_trainable mean just a forward pass jsut to get a sumamry\n",
    "peft_model = peft_model.to('cuda:0')\n",
    "index = 200 ## randomly pick index\n",
    "dialogue = tokenize_datasets['test'][index]['input']\n",
    "human_baseline_summary = tokenize_datasets['test'][index]['output']\n",
    "# dialogue = 'Finance department is important in a company. It is crucial to have a it in all companies. All employees must be aware of this. The finance department must have highly qualified people. It should operate within a company.'\n",
    "prompt = f\"\"\"\n",
    "Give the sentiment of the following statement:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "nSentiment:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "input_ids = input_ids.to('cuda:0')\n",
    "peft_model = peft_model.to('cuda:0')\n",
    "original_model = original_model.to('cuda:0')\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f'Human Baseline: \\n{human_baseline_summary}\\n')\n",
    "print(f'Original Model Output \\n{original_model_text_output}\\n')\n",
    "print(f'Peft Model Output \\n{peft_model_text_output}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
